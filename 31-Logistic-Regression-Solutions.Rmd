# Logistic Regression {#LOGREG}

## Objectives

1) Using `R`, conduct logistic regression and interpret the output and perform model selection.  
2) Write the logistic regression model and predict outputs for given inputs.  
3) Find confidence intervals for parameter estimates and predictions.    
4) Create and interpret a confusion matrix.    

## Homework Problems

### Problem 1

**Possum classification** 

Let's investigate the `possum` data set again. This time we want to model a binary outcome variable. As a reminder, the common brushtail possum of the Australia region is a bit cuter than its distant cousin, the American opossum. We consider 104 brushtail possums from two regions in Australia, where the possums may be considered a random sample from the population. The first region is Victoria, which is in the eastern half of Australia and traverses the southern coast. The second region consists of New South Wales and Queensland, which make up eastern and northeastern Australia.

We use logistic regression to differentiate between possums in these two regions. The outcome variable, called `pop`, takes value `Vic` when a possum is from Victoria and `other` when it is from New South Wales or Queensland. We consider five predictors: `sex`, `head_l`, `skull_w`, `total_l`, and `tail_l`. 

a. Explore the data by making histograms of the quantitative variables, and bar charts of the discrete variables.  Are there any outliers that are likely to have a very large influence on the logistic regression model?  

```{r warning=FALSE,message=FALSE}
possum <- read_csv("data/possum.csv") %>%
  select(pop,sex,head_l,skull_w,total_l,tail_l) %>%
  mutate(pop=factor(pop),sex=factor(sex))
```

```{r warning=FALSE,message=FALSE}
inspect(possum)
```

```{r}
possum %>%
  gf_props(~pop,fill="cyan",color="black") %>%
  gf_theme(theme_bw()) %>%
  gf_labs(x="Population")
```

```{r}
possum %>%
  gf_props(~sex,fill="cyan",color="black") %>%
  gf_theme(theme_bw()) %>%
  gf_labs(x="Gender")
```
```{r}
possum %>%
  gf_boxplot(~head_l) %>%
  gf_theme(theme_bw())
```

```{r}
possum %>%
  gf_boxplot(~skull_w) %>%
  gf_theme(theme_bw())
```

```{r}
possum %>%
  gf_boxplot(~total_l) %>%
  gf_theme(theme_bw())
```

```{r}
possum %>%
  gf_boxplot(~tail_l) %>%
  gf_theme(theme_bw())
```

There are some potential outliers for skull width but otherwise not much concern.

```{r}
pairs(possum[,3:6],lower.panel = panel.smooth)
```
We can see that `head_l` is correlated with the other three variables. This will cause some multicollinearity problems.

b. Build a logistic regression model with all the variable.  Report a summary of the model. 

```{r}
possum_mod <- glm(pop=="Vic"~.,data=possum,family="binomial")
```

```{r}
summary(possum_mod)
```

```{r}
confint(possum_mod)
```


c. Using the p-values decide if you want to remove a variable(S) and if so build that model. 

Let's remove `head_l` first.

```{r}
possum_mod_red <- glm(pop=="Vic"~sex+skull_w+total_l+tail_l,data=possum,family="binomial")
```

```{r}
summary(possum_mod_red)
``` 

Since `head_l` was correlated with the other variables, removing it has increased the precision, decreased the standard error, of the other predictors. There p-values are all now less than 0.05. 

d. For any variable you decide to remove, build a 95% confidence interval for the parameter.

```{r}
confint(possum_mod)
``` 

We are 95\% confident that the true slope coefficient for `head_l` is between -0.44 and 0.108.

The bootstrap is not working for this problem. It may be that we have convergence issues when we resample the data. This is a reminder that we need to be careful and not just run methods without checking results. Here is the code:

```{r warning=FALSE,message=FALSE,cache=TRUE}
set.seed(952)
results<-do(1000)*glm(pop=="Vic"~.,data=resample(possum),family="binomial")
```


```{r}
head(results[,1:5])
```

```{r}
results %>%
  gf_histogram(~head_l,fill="cyan",color = "black") %>%
  gf_theme(theme_bw()) %>%
  gf_labs(title="Bootstrap sampling distribtuion",
          x="sex paramater estimate")
```  


```{r}
cdata(~head_l,data=results)
```

This interval is too large. The resampling process has too much variation in it. This could be due to the small sample size and the multicollinearity.

e. Explain why the remaining parameter estimates change between the two models. 

When coefficient estimates are sensitive to which variables are included in the model, this typically indicates that some variables are collinear. For example, a possum's gender may be related to its head length, which would explain why the coefficient (and p-value) for sex male changed when we removed the head length variable. Likewise, a possum's skull width is likely to be related to its head length, probably even much more closely related than the head length was to gender.


f. Write out the form of the model. Also identify which of the following variables are positively associated (when controlling for other variables) with a possum being from Victoria: `head_l`, `skull_w`, `total_l`, and `tail_l`.  

We dropped `head_l` from the model. Here is the equation:

$$
\log_{e}\left( \frac{p_i}{1-p_i} \right)
	= 33.5 - 1.42 \text{ sex} -0.28 \text{ skull width}  + 0.57 \text{ total length} - 1.81 \text{ tail length}
$$  

Only `total_l` is positively association with the probability of being from Victoria. 

g. Suppose we see a brushtail possum at a zoo in the US, and a sign says the possum had been captured in the wild in Australia, but it doesn't say which part of Australia. However, the sign does indicate that the possum is male, its skull is about 63 mm wide, its tail is 37 cm long, and its total length is 83 cm. What is the reduced model's computed probability that this possum is from Victoria? How confident are you in the model's accuracy of this probability calculation?

Let's predict the outcome. We use `response` for the type to put the answer in the form of a probability. See the help menu on `predict.glm` for more information.  

```{r}
predict(possum_mod_red,newdata = data.frame(sex="m",skull_w=63,tail_l=37,total_l=83),
        type="response",se.fit = TRUE)
```


While the probability, 0.006, is very near zero, we have not run diagnostics on the model. We should also have a little skepticism that the model will hold for a possum found in a US zoo. However, it is encouraging that the possum was caught in the wild.

As a rough sense of the accuracy, we will use the standard error. The errors are really binomial but we are trying to use a normal approximation. If you remember back to our block on probability, with such a low probability, this assumption of normality is suspect. However, we will use it to give us an upper bound.

```{r}
0.0062+c(-1,1)*1.96*.008
```

So at most, the probability of the possum being from Victoria is 2\%.

### Problem 2

**Medical school admission**

The file `MedGPA.csv` in the `data` folder has information on medical school admission status and GPA and standardized test scores gathered on 55 medical school applicants from a liberal arts college in the Midwest.

The variables are:

`Accept Status`: A=accepted to medical school or D=denied admission
`Acceptance`:	Indicator for Accept: 1=accepted or 0=denied  
`Sex`: F=female or M=male  
`BCPM`:	Bio/Chem/Physics/Math grade point average  
`GPA`:	College grade point average  
`VR`:	Verbal reasoning (subscore)  
`PS`:	Physical sciences (subscore)  
`WS`:	Writing sample (subcore)  
`BS`:	Biological sciences (subscore)  
`MCAT`:	Score on the MCAT exam (sum of CR+PS+WS+BS)  
`Apps`:	Number of medical schools applied to  

a. Build a logistic regression model to predict `Acceptance` from `GPA` and `Sex.  

```{r warning=FALSE,message=FALSE}
MedGPA <- read_csv("data/MedGPA.csv")
```

```{r}
glimpse(MedGPA)
```


```{r}
med_mod<-glm(Accept=="D"~GPA+Sex,data=MedGPA,family=binomial)
```

```{r}
summary(med_mod)
```

b. Generate a 95% confidence interval for the coefficient associated with `GPA`. 


```{r}
confint(med_mod)
```  

Let's try the bootstrap for this problem.

```{r cache=TRUE}
set.seed(819)
results_med <- do(1000)*glm(Accept=="D"~GPA+Sex,data=resample(MedGPA),family=binomial)
```

```{r}
results_med %>%
  gf_histogram(~GPA,fill="cyan",color = "black") %>%
  gf_theme(theme_bw()) %>%
  gf_labs(title="Bootstrap sampling distribtuion",
          x="GPA paramater estimate")
``` 

```{r}
cdata(~GPA,data=results_med)
```

This is not so bad. It appears that the distribution of `GPA` is skewed to the left.  

c. Fit a model with a polynomial of degree 2 in the `GPA`. Drop `Sex` from the model. Does a quadratic fit improve the model? 

```{r}
med_mod2<-glm(Accept=="D"~poly(GPA,2),data=MedGPA,family=binomial)
```

```{r}
summary(med_mod2)
```
The quadratic term did not improve the model. 

d. Fit a model with just `GPA` and interpret the coefficient. 
```{r}
tidy(glm(Accept=="D"~GPA,data=MedGPA,family=binomial))
```

```{r}
exp(-5.454166)
```

An increase of 1 in a student's GPA decreases the odds of being denied acceptance by 0.00428. Remember that this is not a probability. As a reminder an odds of $\frac{1}{2}$ means the probability of success is $\frac{1}{3}$. An odds of 1 means a probability of success of $\frac{1}{2}$. Assume the initial odds are 1 if the odds are now 0.00428 smaller, then the probability of success is $\frac{428}{10428}$ or 0.04. The probability decreased by an order of magnitude.

e. Try to add different predictors to come up with your best model. Do not use Acceptance and MCAT in the model.

```{r}
tidy(glm(Accept=="D"~.-Acceptance-MCAT,data=MedGPA,family=binomial)) %>%
  mutate(p.adj=p.adjust(p.value)) %>%
  select(term,p.value,p.adj)
```

Let's take out `VR`.

```{r}
tidy(glm(Accept=="D"~.-Acceptance-MCAT-VR,data=MedGPA,family=binomial)) %>%
  mutate(p.adj=p.adjust(p.value)) %>%
  select(term,p.value,p.adj)
```
Now let's take out `Apps`.

```{r}
tidy(glm(Accept=="D"~.-Acceptance-MCAT-VR-Apps,data=MedGPA,family=binomial)) %>%
  mutate(p.adj=p.adjust(p.value)) %>%
  select(term,p.value,p.adj)
```

Next, let's remove `BCPM`.

```{r}
tidy(glm(Accept=="D"~.-Acceptance-MCAT-VR-Apps-BCPM,
         data=MedGPA,family=binomial)) %>%
  mutate(p.adj=p.adjust(p.value)) %>%
  select(term,p.value,p.adj)
```

Now `WS`.

```{r}
tidy(glm(Accept=="D"~.-Acceptance-MCAT-VR-Apps-BCPM-WS,
         data=MedGPA,family=binomial)) %>%
  mutate(p.adj=p.adjust(p.value)) %>%
  select(term,p.value,p.adj)
```

Maybe `PS`.

```{r}
tidy(glm(Accept=="D"~.-Acceptance-MCAT-VR-Apps-BCPM-WS-PS,
         data=MedGPA,family=binomial)) %>%
  mutate(p.adj=p.adjust(p.value)) %>%
  select(term,p.value,p.adj)
```

We will stop there. There has to be a better way. Machine learning and advanced regression courses explore how to select predictors and improve a model. 

f. Generate a confusion matrix for the best model you have developed.  

```{r}
med_mod2<-glm(Accept=="D"~Sex+GPA+BS,
         data=MedGPA,family=binomial)
```

```{r}
summary(med_mod2)
```



```{r}
augment(med_mod2,type.predict = "response") %>%
  rename(actual=starts_with('Accept ==')) %>%
  transmute(result=as.integer(.fitted>0.5),
            actual=as.integer(actual)) %>%
  table()
```

This model has an accuracy of $\frac{47}{55}$ or 85.5%.


f. Find a 95% confidence interval for the probability a female student with a 3.5 GPA, a `BCPM` of 3.8, a verbal reasoning score of 10, a physical sciences score of 9, a writing sample score of 8, a biological score of 10, a MCAT score of 40, and who applied to 5 medical schools.  

```{r}
predict(med_mod2,newdata = data.frame(Sex="F",
                                      GPA=3.5,
                                      BS=10),
        type="response",se.fit = TRUE)
```  

```{r}
0.2130332+c(-1,1)*1.96*.1122277
```  

For a female students with a GPA of 3.5 and a biological score of 10, we are 95% confident that the probability of being denied acceptance to medical school is between 0 and .433.  

Let's try a bootstrap.

```{r cache=TRUE,warning=FALSE}
set.seed(729)
results <- do(1000)*glm(Accept=="D"~Sex+GPA+BS,
                      data=resample(MedGPA),
                      family=binomial)
```  

```{r}
head(results)
```


```{r}
results_pred <- results %>% 
  mutate(pred=1/(1+exp(-1*(Intercept+3.5*GPA+10*BS))))
```

```{r}
cdata(~pred,data=results_pred)
```

This is close to what we found and does not make the assumption the probability of success is normally distributed. 